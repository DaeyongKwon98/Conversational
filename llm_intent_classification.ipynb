{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "from ast import literal_eval\n",
    "from torch import cuda\n",
    "import json\n",
    "device = 'cuda:0' if cuda.is_available() else 'cpu'\n",
    "\n",
    "class Dataset_768(Dataset):\n",
    "\tdef __init__(self, dataframe):\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.data = dataframe\n",
    "\t\tself.vector = dataframe.vector\n",
    "\t\tself.targets = self.data.intent\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn {\n",
    "\t\t\t'vector': torch.tensor(self.vector, dtype=torch.float),\n",
    "\t\t\t'targets': torch.tensor(self.targets[index], dtype=torch.int)\n",
    "\t\t}\n",
    "\n",
    "# Define Model\n",
    "\n",
    "class DistilBERTClass(nn.Module):\n",
    "\tdef __init__(self, num_intents):\n",
    "\t\tsuper(DistilBERTClass, self).__init__()\n",
    "\t\tself.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\t\tself.fc1 = nn.Sequential(\n",
    "\t\t\tnn.Linear(768, 64),\n",
    "\t\t\tnn.BatchNorm1d(64),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t)\n",
    "\t\tself.fc2 = nn.Sequential(\n",
    "\t\t\tnn.Linear(64, num_intents)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, input_ids, attention_mask):\n",
    "\t\toutput_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\t\thidden_state = output_1[0]\n",
    "\t\tpooler = hidden_state[:, 0]\n",
    "\t\tpooler = self.fc1(pooler)\n",
    "\t\toutput = self.fc2(pooler)\n",
    "\t\treturn output\n",
    "\n",
    "class DistilBERTClass_noFinetune(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistilBERTClass_noFinetune, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1.last_hidden_state # output_1[0]이랑 같음 (아마도)\n",
    "        pooler = hidden_state[:, 0]\n",
    "        return pooler\n",
    "\n",
    "class MLP_768(nn.Module):\n",
    "\tdef __init__(self, num_intents):\n",
    "\t\tsuper(MLP_768, self).__init__()\n",
    "\t\tself.fc1 = nn.Sequential(\n",
    "\t\t\tnn.Linear(768, 64),\n",
    "\t\t\tnn.BatchNorm1d(64),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t)\n",
    "\t\tself.fc2 = nn.Linear(64, num_intents)\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\tx = self.sigmoid(x)\n",
    "\t\treturn x\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBERTClass_noFinetune().to(device)\n",
    "\n",
    "user_intents = ['initial_query', 'greeting', 'add_filter', 'remove_filter', 'continue', 'accept_response', 'reject_response', 'others']\n",
    "system_intents = ['feedback_request', 'detail_attribute_request', 'passive_recommend', 'active_recommend', 'parroting_response', 'sympathetic_response', 'others']\n",
    "music_attributes = ['track', 'artist', 'year', 'popularity', 'culture', 'similar_track', 'similar_artist', 'user', 'theme', 'mood', 'genre', 'instrument', 'vocal', 'tempo', 'none']\n",
    "intents_dict = {'user': user_intents, 'system': system_intents, 'music': music_attributes}\n",
    "\n",
    "df = pd.read_csv('./most_recent.csv', encoding='unicode_escape')\n",
    "df['intent'] = df['intent'].apply(literal_eval)\n",
    "df['music_attribute'] = df['music_attribute'].apply(literal_eval)\n",
    "\n",
    "# df = df[df['dialog_id'].apply(lambda x: x not in error_dialog_id)]\n",
    "\n",
    "# 20개 이하인 intent는 others로 변경 (question 13개, answer 7개)\n",
    "df[\"intent\"] = df[\"intent\"].apply(lambda x: [\"others\" if item in [\"item_attribute_answer\", \"item_attribute_question\"] else item for item in x])\n",
    "\n",
    "# others 외의 intent가 함께 있으면 others 제거\n",
    "def remove_others_if_not_alone(intents):\n",
    "\tif 'others' in intents and len(intents) > 1:\n",
    "\t\tintents.remove('others')\n",
    "\treturn intents\n",
    "df['intent'] = df['intent'].apply(remove_others_if_not_alone)\n",
    "\n",
    "# initial query와 함께 [remove_filter, continue, accept_response, reject_response, others]가 있으면 제거\n",
    "def preprocess_initial(row):\n",
    "\tif 'initial_query' in row['intent']:\n",
    "\t\tfor intent_to_remove in ['remove_filter', 'continue', 'accept_response', 'reject_response', 'others']:\n",
    "\t\t\tif intent_to_remove in row['intent']:\n",
    "\t\t\t\trow['intent'].remove(intent_to_remove)\n",
    "\treturn row\n",
    "df = df.apply(preprocess_initial, axis=1)\n",
    "\n",
    "#######################\n",
    "def concat_previous_1_rows(group):\n",
    "\tif len(group) < 1:\n",
    "\t\treturn pd.DataFrame()\n",
    "\tgroup = group.copy()\n",
    "\tgroup['content'] = group['content'].shift(1).fillna('') + '. ' + group['content']\n",
    "\tgroup['content'].iloc[0] = group['content'].iloc[0].lstrip('. ')\n",
    "\treturn group\n",
    "\n",
    "def concat_previous_2_rows(group):\n",
    "\tif len(group) < 2:\n",
    "\t\treturn pd.DataFrame()\n",
    "\tgroup = group.copy()\n",
    "\tgroup['content'] = group['content'].shift(2).fillna('') + '. ' + group['content'].shift(1).fillna('') + '. ' + group['content']\n",
    "\tgroup['content'].iloc[0] = group['content'].iloc[0].lstrip('. ')\n",
    "\tgroup['content'].iloc[1] = group['content'].iloc[1].lstrip('. ')\n",
    "\treturn group\n",
    "\n",
    "def concat_previous_4_rows(group):\n",
    "\tif len(group) < 4:\n",
    "\t\treturn pd.DataFrame()\n",
    "\tgroup = group.copy()\n",
    "\tgroup['content'] = group['content'].shift(4).fillna('') + '. ' + group['content'].shift(3).fillna('') + '. ' + group['content'].shift(2).fillna('') + '. ' + group['content'].shift(1).fillna('') + '. ' + group['content']\n",
    "\tfor i in range(4):\n",
    "\t\tgroup['content'].iloc[i] = group['content'].iloc[i].lstrip('. ')\n",
    "\treturn group\n",
    "\n",
    "def concat_previous_6_rows(group):\n",
    "\tif len(group) < 6:\n",
    "\t\treturn pd.DataFrame()\n",
    "\tgroup = group.copy()\n",
    "\tgroup['content'] = group['content'].shift(6).fillna('') + '. ' + group['content'].shift(5).fillna('') + '. ' + group['content'].shift(4).fillna('') + '. ' + group['content'].shift(3).fillna('') + '. ' + group['content'].shift(2).fillna('') + '. ' + group['content'].shift(1).fillna('') + '. ' + group['content']\n",
    "\tfor i in range(6):\n",
    "\t\tgroup['content'].iloc[i] = group['content'].iloc[i].lstrip('. ')\n",
    "\treturn group\n",
    "\n",
    "def concat_previous_8_rows(group):\n",
    "\tif len(group) < 8:\n",
    "\t\treturn pd.DataFrame()\n",
    "\tgroup = group.copy()\n",
    "\tgroup['content'] = (\n",
    "\t\tgroup['content'].shift(8).fillna('') + '. ' +\n",
    "\t\tgroup['content'].shift(7).fillna('') + '. ' +\n",
    "\t\tgroup['content'].shift(6).fillna('') + '. ' + \n",
    "\t\tgroup['content'].shift(5).fillna('') + '. ' + \n",
    "\t\tgroup['content'].shift(4).fillna('') + '. ' + \n",
    "\t\tgroup['content'].shift(3).fillna('') + '. ' + \n",
    "\t\tgroup['content'].shift(2).fillna('') + '. ' + \n",
    "\t\tgroup['content'].shift(1).fillna('') + '. ' + \n",
    "\t\tgroup['content']\n",
    "\t)\n",
    "\tfor i in range(8):\n",
    "\t\tgroup['content'].iloc[i] = group['content'].iloc[i].lstrip('. ')\n",
    "\treturn group\n",
    "\n",
    "# 'dialog_id'별로 그룹화하여 이전 n개 row를 concat\n",
    "df_1 = df.groupby('dialog_id').apply(concat_previous_1_rows).reset_index(drop=True)\n",
    "df_2 = df.groupby('dialog_id').apply(concat_previous_2_rows).reset_index(drop=True)\n",
    "df_4 = df.groupby('dialog_id').apply(concat_previous_4_rows).reset_index(drop=True)\n",
    "df_6 = df.groupby('dialog_id').apply(concat_previous_6_rows).reset_index(drop=True)\n",
    "df_8 = df.groupby('dialog_id').apply(concat_previous_8_rows).reset_index(drop=True)\n",
    "\n",
    "df = df\n",
    "################################\n",
    "# Define Dataset Class\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "\tdef __init__(self, dataframe, tokenizer, max_len):\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.data = dataframe\n",
    "\t\tself.text = dataframe.content\n",
    "\t\tself.targets = self.data.intent\n",
    "\t\tself.max_len = max_len\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.text)\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\ttext = str(self.text[index])\n",
    "\t\ttext = \" \".join(text.split())\n",
    "\n",
    "\t\ttokens = self.tokenizer.tokenize(text)\n",
    "\t\tif len(tokens) > self.max_len:\n",
    "\t\t\ttokens = tokens[-self.max_len:]\n",
    "\t\ttruncated_text = self.tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "\t\tinputs = self.tokenizer.encode_plus(\n",
    "\t\t\ttruncated_text,\n",
    "\t\t\tNone,\n",
    "\t\t\tadd_special_tokens=True,\n",
    "\t\t\tmax_length=self.max_len,\n",
    "\t\t\tpad_to_max_length=True,\n",
    "\t\t\treturn_token_type_ids=False,\n",
    "   \t\t\ttruncation=True\n",
    "\t\t)\n",
    "\t\tids = inputs['input_ids']\n",
    "\t\tmask = inputs['attention_mask']\n",
    "\n",
    "\t\treturn {\n",
    "\t\t\t'ids': torch.tensor(ids, dtype=torch.int),\n",
    "\t\t\t'mask': torch.tensor(mask, dtype=torch.int),\n",
    "\t\t\t'targets': torch.tensor(self.targets[index], dtype=torch.int)\n",
    "\t\t}\n",
    "\n",
    "# Define functions\n",
    "\n",
    "def decode_intents(encoded_list):\n",
    "\treturn [intent for intent, flag in zip(intents_dict[data_type], encoded_list) if flag == 1]\n",
    "\n",
    "def text_to_768(text):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    ids = inputs['input_ids'].to(device)\n",
    "    mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(ids, mask)\n",
    "    \n",
    "    return output.cpu().numpy().flatten()\n",
    "\n",
    "df['vector'] = df['content'].apply(text_to_768)\n",
    "\n",
    "##########################\n",
    "\n",
    "user_df = df[df['role']=='user']\n",
    "system_df = df[df['role']=='system']\n",
    "\n",
    "del user_df['role']\n",
    "del user_df['music_attribute']\n",
    "del system_df['role']\n",
    "del system_df['music_attribute']\n",
    "\n",
    "def encode_intents(intent_list, intents):\n",
    "\treturn [1 if intent in intent_list else 0 for intent in intents]\n",
    "\n",
    "user_df.loc[:, 'intent'] = user_df['intent'].apply(lambda x: encode_intents(x, user_intents))\n",
    "user_df = user_df.reset_index(drop=True)\n",
    "\n",
    "system_df.loc[:, 'intent'] = system_df['intent'].apply(lambda x: encode_intents(x, system_intents))\n",
    "system_df = system_df.reset_index(drop=True)\n",
    "\n",
    "music_df = df[['index','dialog_id', 'role', 'content', 'music_attribute']]\n",
    "music_df.loc[:, 'music_attribute'] = music_df['music_attribute'].apply(lambda x: encode_intents(x, music_attributes))\n",
    "music_df.rename(columns={'music_attribute': 'intent'}, inplace=True)\n",
    "music_df = music_df.reset_index(drop=True)\n",
    "\n",
    "user_y = torch.stack([torch.tensor(item) for item in user_df['intent']])\n",
    "system_y = torch.stack([torch.tensor(item) for item in system_df['intent']])\n",
    "music_y = torch.stack([torch.tensor(item) for item in music_df['intent']])\n",
    "\n",
    "# Train, Valid Split\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in msss.split(user_df['content'].values, user_y):\n",
    "\tuser_train_df, user_val_df = user_df.iloc[train_index], user_df.iloc[test_index]\n",
    "\tuser_train_y, user_val_y = user_y[train_index], user_y[test_index]\n",
    "\n",
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "\n",
    "for train_index, test_index in msss.split(user_val_df['content'].values, user_val_y):\n",
    "\tuser_val_df, user_test_df = user_val_df.iloc[train_index], user_val_df.iloc[test_index]\n",
    "\tuser_val_y, user_test_y = user_val_y[train_index], user_val_y[test_index]\n",
    "\n",
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in msss.split(system_df['content'].values, system_y):\n",
    "\tsystem_train_df, system_val_df = system_df.iloc[train_index], system_df.iloc[test_index]\n",
    "\tsystem_train_y, system_val_y = system_y[train_index], system_y[test_index]\n",
    "\n",
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "\n",
    "for train_index, test_index in msss.split(system_val_df['content'].values, system_val_y):\n",
    "\tsystem_val_df, system_test_df = system_val_df.iloc[train_index], system_val_df.iloc[test_index]\n",
    "\tsystem_val_y, system_test_y = system_val_y[train_index], system_val_y[test_index]\n",
    " \n",
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in msss.split(music_df['content'].values, music_y):\n",
    "\tmusic_train_df, music_val_df = music_df.iloc[train_index], music_df.iloc[test_index]\n",
    "\tmusic_train_y, music_val_y = music_y[train_index], music_y[test_index]\n",
    "\n",
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "\n",
    "for train_index, test_index in msss.split(music_val_df['content'].values, music_val_y):\n",
    "\tmusic_val_df, music_test_df = music_val_df.iloc[train_index], music_val_df.iloc[test_index]\n",
    "\tmusic_val_y, music_test_y = music_val_y[train_index], music_val_y[test_index]\n",
    "\n",
    "user_train_df = user_train_df.reset_index(drop=True)\n",
    "user_val_df = user_val_df.reset_index(drop=True)\n",
    "user_test_df = user_test_df.reset_index(drop=True)\n",
    "\n",
    "system_train_df = system_train_df.reset_index(drop=True)\n",
    "system_val_df = system_val_df.reset_index(drop=True)\n",
    "system_test_df = system_test_df.reset_index(drop=True)\n",
    "\n",
    "music_train_df = music_train_df.reset_index(drop=True)\n",
    "music_val_df = music_val_df.reset_index(drop=True)\n",
    "music_test_df = music_test_df.reset_index(drop=True)\n",
    "\n",
    "# Generate Data Dictionary\n",
    "\n",
    "data_dict = {\n",
    "\t'user': {\n",
    "\t\t'train': {\n",
    "\t\t\t'dataframe': user_train_df,\n",
    "\t\t\t'label': user_train_y\n",
    "\t\t},\n",
    "\t\t'val': {\n",
    "\t\t\t'dataframe': user_val_df,\n",
    "\t\t\t'label': user_val_y\n",
    "\t\t},\n",
    "  \t\t'test': {\n",
    "\t\t\t'dataframe': user_test_df,\n",
    "\t\t\t'label': user_test_y\n",
    "\t\t}\n",
    "\t},\n",
    "\t'system': {\n",
    "\t\t'train': {\n",
    "\t\t\t'dataframe': system_train_df,\n",
    "\t\t\t'label': system_train_y\n",
    "\t\t},\n",
    "\t\t'val': {\n",
    "\t\t\t'dataframe': system_val_df,\n",
    "\t\t\t'label': system_val_y\n",
    "\t\t},\n",
    "  \t\t'test': {\n",
    "\t\t\t'dataframe': system_test_df,\n",
    "\t\t\t'label': system_test_y\n",
    "\t\t}\n",
    "\t},\n",
    "\t'music': {\n",
    "\t\t'train': {\n",
    "\t\t\t'dataframe': music_train_df,\n",
    "\t\t\t'label': music_train_y\n",
    "\t\t},\n",
    "\t\t'val': {\n",
    "\t\t\t'dataframe': music_val_df,\n",
    "\t\t\t'label': music_val_y\n",
    "\t\t},\n",
    "  \t\t'test': {\n",
    "\t\t\t'dataframe': music_test_df,\n",
    "\t\t\t'label': music_test_y\n",
    "\t\t}\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>dialog_id</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>content</th>\n",
       "      <th>intent</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>b8e368058df16085</td>\n",
       "      <td>b8e368058df16085:3:user</td>\n",
       "      <td>Thank you</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[-0.077586114, 0.07368618, 0.06842183, -0.0907...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>dac004c6ae9787b2</td>\n",
       "      <td>dac004c6ae9787b2:0:user</td>\n",
       "      <td>Hi, I'd love to create a playlist of Jazz songs</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[-0.047902066, -0.10909475, -0.010763254, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>dac004c6ae9787b2</td>\n",
       "      <td>dac004c6ae9787b2:1:user</td>\n",
       "      <td>Yes, from 2020 please</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[-0.15013573, -0.29752833, 0.27389917, -0.3062...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>b8a2eeb06bc2a045</td>\n",
       "      <td>b8a2eeb06bc2a045:2:user</td>\n",
       "      <td>Great songs, could we get some more like this?</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1, 0, 0]</td>\n",
       "      <td>[0.14025909, -0.26175424, 0.082671985, -0.0402...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58</td>\n",
       "      <td>b8a2eeb06bc2a045</td>\n",
       "      <td>b8a2eeb06bc2a045:3:user</td>\n",
       "      <td>Great! could we get some more like Vampire Wee...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0.23933716, -0.1676852, 0.1416594, -0.1552548...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>9646</td>\n",
       "      <td>41ed9c8c163f6a70</td>\n",
       "      <td>41ed9c8c163f6a70:2:user</td>\n",
       "      <td>These are nice! Got any songs from H.E.R?</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[-0.020169634, -0.3400174, -0.033068836, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>9666</td>\n",
       "      <td>1dee3e19595f524c</td>\n",
       "      <td>1dee3e19595f524c:1:user</td>\n",
       "      <td>I mostly listen to classical music to help me ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0.018026177, 0.0075391917, -0.15731312, -0.17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>9692</td>\n",
       "      <td>d9134f35c8a3a787</td>\n",
       "      <td>d9134f35c8a3a787:2:user</td>\n",
       "      <td>Oh these are good. Can we add some songs from ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0.07373679, -0.07481862, -0.067963, -0.002229...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>9702</td>\n",
       "      <td>6206cf8dccd420fb</td>\n",
       "      <td>6206cf8dccd420fb:2:user</td>\n",
       "      <td>nice please more</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1, 0, 0]</td>\n",
       "      <td>[-0.08899486, -0.11149166, 0.13286069, -0.1453...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>9712</td>\n",
       "      <td>bd2d851f17071b36</td>\n",
       "      <td>bd2d851f17071b36:4:user</td>\n",
       "      <td>yes</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[-0.20539285, -0.11370726, 0.13045602, -0.1963...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>463 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index         dialog_id                unique_id  \\\n",
       "0        8  b8e368058df16085  b8e368058df16085:3:user   \n",
       "1       32  dac004c6ae9787b2  dac004c6ae9787b2:0:user   \n",
       "2       34  dac004c6ae9787b2  dac004c6ae9787b2:1:user   \n",
       "3       56  b8a2eeb06bc2a045  b8a2eeb06bc2a045:2:user   \n",
       "4       58  b8a2eeb06bc2a045  b8a2eeb06bc2a045:3:user   \n",
       "..     ...               ...                      ...   \n",
       "458   9646  41ed9c8c163f6a70  41ed9c8c163f6a70:2:user   \n",
       "459   9666  1dee3e19595f524c  1dee3e19595f524c:1:user   \n",
       "460   9692  d9134f35c8a3a787  d9134f35c8a3a787:2:user   \n",
       "461   9702  6206cf8dccd420fb  6206cf8dccd420fb:2:user   \n",
       "462   9712  bd2d851f17071b36  bd2d851f17071b36:4:user   \n",
       "\n",
       "                                               content  \\\n",
       "0                                            Thank you   \n",
       "1      Hi, I'd love to create a playlist of Jazz songs   \n",
       "2                                Yes, from 2020 please   \n",
       "3       Great songs, could we get some more like this?   \n",
       "4    Great! could we get some more like Vampire Wee...   \n",
       "..                                                 ...   \n",
       "458          These are nice! Got any songs from H.E.R?   \n",
       "459  I mostly listen to classical music to help me ...   \n",
       "460  Oh these are good. Can we add some songs from ...   \n",
       "461                                   nice please more   \n",
       "462                                                yes   \n",
       "\n",
       "                       intent  \\\n",
       "0    [0, 0, 0, 0, 0, 1, 0, 0]   \n",
       "1    [1, 1, 1, 0, 0, 0, 0, 0]   \n",
       "2    [0, 0, 1, 0, 0, 0, 0, 0]   \n",
       "3    [0, 0, 0, 0, 1, 1, 0, 0]   \n",
       "4    [0, 0, 1, 0, 0, 1, 0, 0]   \n",
       "..                        ...   \n",
       "458  [0, 0, 1, 0, 0, 1, 0, 0]   \n",
       "459  [0, 0, 1, 0, 0, 0, 0, 0]   \n",
       "460  [0, 0, 1, 0, 0, 1, 0, 0]   \n",
       "461  [0, 0, 0, 0, 1, 1, 0, 0]   \n",
       "462  [0, 0, 0, 0, 0, 1, 0, 0]   \n",
       "\n",
       "                                                vector  \n",
       "0    [-0.077586114, 0.07368618, 0.06842183, -0.0907...  \n",
       "1    [-0.047902066, -0.10909475, -0.010763254, -0.1...  \n",
       "2    [-0.15013573, -0.29752833, 0.27389917, -0.3062...  \n",
       "3    [0.14025909, -0.26175424, 0.082671985, -0.0402...  \n",
       "4    [0.23933716, -0.1676852, 0.1416594, -0.1552548...  \n",
       "..                                                 ...  \n",
       "458  [-0.020169634, -0.3400174, -0.033068836, -0.02...  \n",
       "459  [0.018026177, 0.0075391917, -0.15731312, -0.17...  \n",
       "460  [0.07373679, -0.07481862, -0.067963, -0.002229...  \n",
       "461  [-0.08899486, -0.11149166, 0.13286069, -0.1453...  \n",
       "462  [-0.20539285, -0.11370726, 0.13045602, -0.1963...  \n",
       "\n",
       "[463 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distilbert로 768차원 Embedding 뽑아서 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden state torch.Size([64, 128, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "params = {'batch_size': BATCH_SIZE, 'shuffle': False, 'num_workers': 4}\n",
    "num_intents_dict = {'user': 8, 'system': 7, 'music': 15}\n",
    "loss_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "for data_type in ['music']:\n",
    "\ttraining_set = MultiLabelDataset(data_dict[data_type]['train']['dataframe'], tokenizer, MAX_LEN)\n",
    "\tvalid_set = MultiLabelDataset(data_dict[data_type]['val']['dataframe'], tokenizer, MAX_LEN)\n",
    "\ttest_set = MultiLabelDataset(data_dict[data_type]['test']['dataframe'], tokenizer, MAX_LEN)\t\n",
    " \n",
    "\ttraining_loader = DataLoader(training_set, **params)\n",
    "\tvalid_loader = DataLoader(valid_set, **params)\n",
    "\ttest_loader = DataLoader(test_set, **params)\n",
    " \n",
    "\tnum_intents = num_intents_dict[data_type]\n",
    " \n",
    "\tmodel = DistilBERTClass_noFinetune(num_intents)\n",
    "\tmodel.to(device)\n",
    "\tmodel.eval()\n",
    "\n",
    "\ttrain_vectors_list = []\n",
    "\tval_vectors_list = []\n",
    "\ttest_vectors_list = []\n",
    " \n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, data in tqdm(enumerate(training_loader, 0), leave=False):\n",
    "\t\t\tids = data['ids'].to(device, dtype = torch.int)\n",
    "\t\t\tmask = data['mask'].to(device, dtype = torch.int)\n",
    "\t\t\toutputs = model(ids, mask)\n",
    "\t\t\ttrain_vectors_list.append(outputs)\n",
    "\t\tfor i, data in tqdm(enumerate(valid_loader, 0), leave=False):\n",
    "\t\t\tids = data['ids'].to(device, dtype = torch.int)\n",
    "\t\t\tmask = data['mask'].to(device, dtype = torch.int)\n",
    "\t\t\toutputs = model(ids, mask)\n",
    "\t\t\tval_vectors_list.append(outputs)\n",
    "\t\tfor i, data in tqdm(enumerate(test_loader, 0), leave=False):\n",
    "\t\t\tids = data['ids'].to(device, dtype = torch.int)\n",
    "\t\t\tmask = data['mask'].to(device, dtype = torch.int)\n",
    "\t\t\toutputs = model(ids, mask)\n",
    "\t\t\ttest_vectors_list.append(outputs)\n",
    "\t\t\tbreak\n",
    "\n",
    "\t# train_vector = torch.cat(train_vectors_list, dim=0)\n",
    "\t# val_vector = torch.cat(val_vectors_list, dim=0)\n",
    "\t# test_vector = torch.cat(test_vectors_list, dim=0)\n",
    "\t# print(train_vector.shape, val_vector.shape, test_vector.shape)\n",
    " \n",
    "\t# torch.save(train_vector, f'./distilbert_768vectors/{data_type}_train.pt')\n",
    "\t# torch.save(val_vector, f'./distilbert_768vectors/{data_type}_val.pt')\n",
    "\t# torch.save(test_vector, f'./distilbert_768vectors/{data_type}_test.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best threshold 얻어서 최종 결과 확인하기 (학습된 모델 불러와서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "params = {'batch_size': BATCH_SIZE, 'shuffle': False, 'num_workers': 4}\n",
    "num_intents_dict = {'user': 8, 'system': 7, 'music': 15}\n",
    "loss_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "for data_type in ['music']:\n",
    "\ttraining_set = MultiLabelDataset(data_dict[data_type]['train']['dataframe'], tokenizer, MAX_LEN)\n",
    "\tvalid_set = MultiLabelDataset(data_dict[data_type]['val']['dataframe'], tokenizer, MAX_LEN)\n",
    "\ttest_set = MultiLabelDataset(data_dict[data_type]['test']['dataframe'], tokenizer, MAX_LEN)\t\n",
    " \n",
    "\ttraining_loader = DataLoader(training_set, **params)\n",
    "\tvalid_loader = DataLoader(valid_set, **params)\n",
    "\ttest_loader = DataLoader(test_set, **params)\n",
    " \n",
    "\tnum_intents = num_intents_dict[data_type]\n",
    " \n",
    "\t# Load Best Model\n",
    "\tmodel = DistilBERTClass(num_intents)\n",
    "\tmodel.load_state_dict(torch.load('./models/' + data_type + '_model.pth'))\n",
    "\tmodel.to(device)\n",
    "\n",
    " \t# Test with threshold = 0.5\n",
    "\tmodel.eval()\n",
    "\tprobability_outputs=[]\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, data in tqdm(enumerate(test_loader, 0), leave=False):\n",
    "\t\t\tids = data['ids'].to(device, dtype = torch.int)\n",
    "\t\t\tmask = data['mask'].to(device, dtype = torch.int)\n",
    "\t\t\ttargets = data['targets'].to(device, dtype = torch.float)\n",
    "\t\t\toutputs = model(ids, mask)\n",
    "\t\t\tprobability_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "\tprobability_outputs = np.array(probability_outputs)\n",
    "   \n",
    "\tbinary_outputs = (probability_outputs >= 0.5)\n",
    "\tbinary_outputs[np.all(binary_outputs == False, axis=1), -1] = True\n",
    "\n",
    "\t# Calculate Result\n",
    "\treport = classification_report(data_dict[data_type]['test']['label'], binary_outputs, output_dict=True)\n",
    "\tdf_report = pd.DataFrame(report).transpose()\n",
    "\tdf_report.reset_index(inplace=True)\n",
    "\tdf_report.rename(columns={'support': 'tag_count', 'index': 'tag'}, inplace=True)\n",
    "\tdf_report.loc[:num_intents-1, 'tag'] = intents_dict[data_type]\n",
    "\tdf_report[['precision', 'recall', 'f1-score']] = df_report[['precision', 'recall', 'f1-score']].round(4)\n",
    "\n",
    "\tprint(df_report['f1-score'].values)\n",
    "\n",
    "\t# Find best threshold with validation set\n",
    "\tmodel.eval()\n",
    "\tlabels = []\n",
    "\tprobability_outputs=[]\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, data in tqdm(enumerate(valid_loader, 0), leave=False):\n",
    "\t\t\tids = data['ids'].to(device, dtype = torch.int)\n",
    "\t\t\tmask = data['mask'].to(device, dtype = torch.int)\n",
    "\t\t\ttargets = data['targets'].to(device, dtype = torch.float)\n",
    "\t\t\toutputs = model(ids, mask)\n",
    "\t\t\tlabels.extend(targets.cpu().detach().numpy().tolist())\n",
    "\t\t\tprobability_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "\n",
    "\tlabels = np.array(labels)\n",
    "\tprobability_outputs = np.array(probability_outputs)\n",
    "\n",
    "\tbest_thresholds = np.zeros(num_intents)\n",
    "\tthresholds = [(i+1)/100 for i in range(100)]\n",
    "\tfor label_idx in range(num_intents):\n",
    "\t\tbest_label_f1 = 0.0\n",
    "\t\tfor threshold in thresholds:\n",
    "\t\t\tbinary_preds = (probability_outputs[:, label_idx] >= threshold).astype(int)\n",
    "\t\t\tf1 = f1_score(labels[:, label_idx], binary_preds)\n",
    "\t\t\tif f1 > best_label_f1:\n",
    "\t\t\t\tbest_label_f1 = f1\n",
    "\t\t\t\tbest_thresholds[label_idx] = threshold\n",
    "\n",
    "\tprint(best_thresholds)\n",
    "\t\n",
    "\t# Test with best thresholds\n",
    "\tmodel.eval()\n",
    "\tlabels = []\n",
    "\tprobability_outputs=[]\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, data in tqdm(enumerate(test_loader, 0), leave=False):\n",
    "\t\t\tids = data['ids'].to(device, dtype = torch.int)\n",
    "\t\t\tmask = data['mask'].to(device, dtype = torch.int)\n",
    "\t\t\ttargets = data['targets'].to(device, dtype = torch.float)\n",
    "\t\t\toutputs = model(ids, mask)\n",
    "\t\t\tlabels.extend(targets.cpu().detach().numpy().tolist())\n",
    "\t\t\tprobability_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "\n",
    "\tlabels = np.array(labels)\n",
    "\tprobability_outputs = np.array(probability_outputs)\n",
    " \n",
    "\tbinary_outputs = (probability_outputs >= best_thresholds).astype(int)\n",
    "\tbinary_outputs[np.all(binary_outputs == False, axis=1), -1] = True\n",
    " \n",
    "\treport = classification_report(labels[:, :-1], binary_outputs[:, :-1], output_dict=True)\n",
    "\tdf_report = pd.DataFrame(report).transpose()\n",
    "\tdf_report.reset_index(inplace=True)\n",
    "\tdf_report.rename(columns={'support': 'tag_count', 'index': 'tag'}, inplace=True)\n",
    "\t#print(df_report.loc[:num_intents-2, 'tag'] )\n",
    "\tdf_report.loc[:num_intents-2, 'tag'] = intents_dict[data_type][:-1]\n",
    "\tdf_report[['precision', 'recall', 'f1-score']] = df_report[['precision', 'recall', 'f1-score']].round(2)\n",
    "\tdf_report = df_report[df_report['tag'].apply(lambda x: x not in ['micro avg', 'weighted avg', 'samples avg'])]\n",
    "\n",
    "df_report[['precision', 'recall', 'f1-score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCH = 15\n",
    "patience = 2\n",
    "count = 0\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "params = {'batch_size': BATCH_SIZE, 'shuffle': False, 'num_workers': 4}\n",
    "num_intents_dict = {'user': 8, 'system': 7, 'music': 15}\n",
    "\n",
    "for data_type in ['music']:\n",
    "\ttraining_set = MultiLabelDataset(data_dict[data_type]['train']['dataframe'], tokenizer, MAX_LEN)\n",
    "\tvalid_set = MultiLabelDataset(data_dict[data_type]['val']['dataframe'], tokenizer, MAX_LEN)\n",
    "\ttest_set = MultiLabelDataset(data_dict[data_type]['test']['dataframe'], tokenizer, MAX_LEN)\t\n",
    " \n",
    "\ttraining_loader = DataLoader(training_set, **params)\n",
    "\tvalid_loader = DataLoader(valid_set, **params)\n",
    "\ttest_loader = DataLoader(test_set, **params)\n",
    "\n",
    "\tnum_intents = num_intents_dict[data_type]\n",
    "\tmodel = DistilBERTClass(num_intents)\n",
    "\tmodel.to(device)\n",
    "\n",
    "\toptimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\tscheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\tloss_criterion = torch.nn.BCEWithLogitsLoss()\n",
    " \n",
    "\ttrain_loss_list = []\n",
    "\tvalid_loss_list = []\t\n",
    "\n",
    "\tmin_loss = 100.0\n",
    "\t# Train\n",
    "\tprint(\"Training Start\")\n",
    "\tfor epoch in range(NUM_EPOCH):\n",
    "\t\ttrain_loss = 0.0\n",
    "\t\tmodel.train()\n",
    "\t\tfor i,data in tqdm(enumerate(training_loader, 0), leave=False):\n",
    "\t\t\tids = data['ids'].to(device, dtype = torch.int)\n",
    "\t\t\tmask = data['mask'].to(device, dtype = torch.int)\n",
    "\t\t\ttargets = data['targets'].to(device, dtype = torch.float)\n",
    "\t\t\toutputs = model(ids, mask)\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss = loss_criterion(outputs, targets)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\ttrain_loss += loss.item()\n",
    "\t\tscheduler.step()\n",
    "\t\ttrain_loss /= i\n",
    "\t\n",
    "\t\t# Validation\n",
    "\t\tmodel.eval()\n",
    "\t\tprobability_outputs=[]\n",
    "\t\tvalid_loss = 0.0\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor i, data in tqdm(enumerate(valid_loader, 0), leave=False):\n",
    "\t\t\t\tids = data['ids'].to(device, dtype = torch.int)\n",
    "\t\t\t\tmask = data['mask'].to(device, dtype = torch.int)\n",
    "\t\t\t\ttargets = data['targets'].to(device, dtype = torch.float)\n",
    "\t\t\t\toutputs = model(ids, mask)\n",
    "\t\t\t\tvalid_loss += loss_criterion(outputs, targets).item()\n",
    "\t\t\tvalid_loss /= i\n",
    "\t\tprint(f\"Epoch:{epoch+1}, Train Loss: {round(train_loss,4)}, Valid Loss: {round(valid_loss,4)}, lr: {LEARNING_RATE*(0.9)**epoch}\")\n",
    "\t\ttrain_loss_list.append(round(train_loss,4))\n",
    "\t\tvalid_loss_list.append(round(valid_loss,4))\n",
    " \n",
    "\t\t# # Save model\n",
    "\t\t# if valid_loss < min_loss:\n",
    "\t\t# \tmin_loss = valid_loss\n",
    "\t\t# \tcount = 0\n",
    "   \n",
    "\t\t# \tmodel_name = f\"./models/{data_type}_model_concatone.pth\"\n",
    "\t\t# \ttorch.save(model.state_dict(), model_name)\n",
    "\t\t# \tprint(f\"{data_type} model saved with valid loss {round(min_loss,4)}\")\n",
    "\t\t# else:\n",
    "\t\t# \tcount += 1\n",
    "\t\t# \tif count==patience: break\n",
    " \n",
    "\t# # Load Best Model\n",
    "\t# model = DistilBERTClass(num_intents)\n",
    "\t# model.load_state_dict(torch.load(f\"./models/{data_type}_model_concatone.pth\"))\n",
    "\t# model.to(device)\n",
    "\n",
    "\t# # Test with best model\n",
    "\t# print(\"Test Start\")\n",
    "\t# test_loss = 0.0\n",
    "\t# model.eval()\n",
    "\t# probability_outputs=[]\n",
    "\t# with torch.no_grad():\n",
    "\t# \tfor i, data in tqdm(enumerate(test_loader, 0), leave=False):\n",
    "\t# \t\tids = data['ids'].to(device, dtype = torch.int)\n",
    "\t# \t\tmask = data['mask'].to(device, dtype = torch.int)\n",
    "\t# \t\ttargets = data['targets'].to(device, dtype = torch.float)\n",
    "\t# \t\toutputs = model(ids, mask)\n",
    "\t# \t\ttest_loss += loss_criterion(outputs, targets).item()\n",
    "\t# \t\tprobability_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "\t# \ttest_loss /= i\n",
    "\t# \tprint(f\"Test loss: {round(test_loss,4)}\")\n",
    "\t# probability_outputs = np.array(probability_outputs)\n",
    "\t# #np.save('./probs/' + data_type + '_concat.npy', probability_outputs)\n",
    "    \n",
    "\t# binary_outputs = (probability_outputs >= 0.5)\n",
    "\t# binary_outputs[np.all(binary_outputs == False, axis=1), -1] = True # 모두 False인 경우 Others로 Labeling\n",
    "\n",
    "\t# # Calculate Result\n",
    "\t# report = classification_report(data_dict[data_type]['test']['label'], binary_outputs, output_dict=True)\n",
    "\t# df_report = pd.DataFrame(report).transpose()\n",
    "\t# df_report.reset_index(inplace=True)\n",
    "\t# df_report.rename(columns={'support': 'tag_count', 'index': 'tag'}, inplace=True)\n",
    "\t# df_report.loc[:num_intents-1, 'tag'] = intents_dict[data_type]\n",
    "\t# df_report[['precision', 'recall', 'f1-score']] = df_report[['precision', 'recall', 'f1-score']].round(4)\n",
    "\t# df_report['tag_count'] = df_report['tag_count'].astype(int)\n",
    "\t# # Save classification report\n",
    "\t# df_report.to_csv(f\"./results/{data_type}_concateight.csv\", index=False)\n",
    "\n",
    "\t# Loss Plot\n",
    "\tepochs = range(1, len(train_loss_list)+1)\n",
    "\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.plot(epochs, train_loss_list, 'o-', label='Train Loss')\n",
    "\tplt.plot(epochs, valid_loss_list, 's-', label='Valid Loss')\n",
    "\tplt.xlabel('Epochs')\n",
    "\tplt.ylabel('Loss')\n",
    "\tplt.title('Train, Valid Loss over Epochs')\n",
    "\tplt.legend()\n",
    "\t#plt.savefig(f\"./{data_type}.jpg\")\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 768 벡터로 MLP Layer만 학습하기 (for 부분 고쳐야됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-2\n",
    "NUM_EPOCH = 15\n",
    "patience = 2\n",
    "count = 0\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "num_intents_dict = {'user': 8, 'system': 7, 'music': 15}\n",
    "\n",
    "for data_type in ['music']:\n",
    "\ttrain_vector = torch.load(f'./distilbert_768vectors/{data_type}_train.pt').to(device)\n",
    "\tval_vector = torch.load(f'./distilbert_768vectors/{data_type}_val.pt').to(device)\n",
    "\ttest_vector = torch.load(f'./distilbert_768vectors/{data_type}_test.pt').to(device)\n",
    " \n",
    "\ttrain_y = torch.tensor(data_dict[data_type]['train']['label'], dtype=torch.float32).to(device)\n",
    "\tval_y = torch.tensor(data_dict[data_type]['val']['label'], dtype=torch.float32).to(device)\n",
    "\ttest_y = torch.tensor(data_dict[data_type]['test']['label'], dtype=torch.float32).to(device)\n",
    "\n",
    "\ttrain_dataset = TensorDataset(train_vector, train_y)\n",
    "\tval_dataset = TensorDataset(val_vector, val_y)\n",
    "\ttest_dataset = TensorDataset(test_vector, test_y)\n",
    "\n",
    "\ttrain_dataloader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\tval_dataloader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\ttest_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\tnum_intents = num_intents_dict[data_type]\n",
    "\tmodel = MLP_768(num_intents)\n",
    "\tmodel.to(device)\n",
    "\n",
    "\toptimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\tscheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\tloss_criterion = torch.nn.BCEWithLogitsLoss()\n",
    " \n",
    "\ttrain_loss_list = []\n",
    "\tvalid_loss_list = []\t\n",
    "\n",
    "\tmin_loss = 100.0\n",
    "\t# Train\n",
    "\tprint(\"Training Start\")\n",
    "\tfor epoch in range(NUM_EPOCH):\n",
    "\t\ttrain_loss = 0.0\n",
    "\t\tmodel.train()\n",
    "  \n",
    "\t\tfor i, data in tqdm(enumerate(train_dataloader, 0), leave=False):\n",
    "\t\t\tids = data['ids'].to(device, dtype = torch.int)\n",
    "\t\t\tmask = data['mask'].to(device, dtype = torch.int)\n",
    "\t\t\ttargets = data['targets'].to(device, dtype = torch.float)\n",
    "\t\t\toutputs = model(ids, mask)\n",
    "\t\t\tprint(outputs.shape)\n",
    "\t\t\toutputs = model(outputs)\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss = loss_criterion(outputs, targets)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\ttrain_loss += loss.item()\n",
    "\t\tscheduler.step()\n",
    "\t\ttrain_loss /= i\n",
    "\t\n",
    "\t\t# Validation\n",
    "\t\tmodel.eval()\n",
    "\t\tprobability_outputs=[]\n",
    "\t\tvalid_loss = 0.0\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor i, data in tqdm(enumerate(val_dataloader, 0), leave=False):\n",
    "\t\t\t\tids = data['ids'].to(device, dtype = torch.int)\n",
    "\t\t\t\tmask = data['mask'].to(device, dtype = torch.int)\n",
    "\t\t\t\ttargets = data['targets'].to(device, dtype = torch.float)\n",
    "\t\t\t\toutputs = model(ids, mask)\n",
    "\t\t\t\tvalid_loss += loss_criterion(outputs, targets).item()\n",
    "\t\t\tvalid_loss /= i\n",
    "\t\tprint(f\"Epoch:{epoch+1}, Train Loss: {round(train_loss,4)}, Valid Loss: {round(valid_loss,4)}, lr: {LEARNING_RATE*(0.9)**epoch}\")\n",
    "\t\ttrain_loss_list.append(round(train_loss,4))\n",
    "\t\tvalid_loss_list.append(round(valid_loss,4))\n",
    " \n",
    "\t\t# Save model\n",
    "\t\tif valid_loss < min_loss:\n",
    "\t\t\tmin_loss = valid_loss\n",
    "\t\t\tcount = 0\n",
    "   \n",
    "\t\t\tmodel_name = f\"./models/{data_type}_model_768.pth\"\n",
    "\t\t\ttorch.save(model.state_dict(), model_name)\n",
    "\t\t\tprint(f\"{data_type} model saved with valid loss {round(min_loss,4)}\")\n",
    "\t\telse:\n",
    "\t\t\tcount += 1\n",
    "\t\t\tif count==patience: break\n",
    "\n",
    "\t# Loss Plot\n",
    "\tepochs = range(1, len(train_loss_list)+1)\n",
    "\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.plot(epochs, train_loss_list, 'o-', label='Train Loss')\n",
    "\tplt.plot(epochs, valid_loss_list, 's-', label='Valid Loss')\n",
    "\tplt.xlabel('Epochs')\n",
    "\tplt.ylabel('Loss')\n",
    "\tplt.title(f'{data_type} Loss')\n",
    "\tplt.legend()\n",
    "\t#plt.savefig(f\"./{data_type}.jpg\")\n",
    "\tplt.show()\n",
    "\n",
    "\t# Load Best Model\n",
    "\tmodel = MLP_768(num_intents)\n",
    "\tmodel.load_state_dict(torch.load('./models/' + data_type + '_model_768.pth'))\n",
    "\tmodel.to(device)\n",
    "\n",
    "\t# # Test with threshold = 0.5\n",
    "\t# model.eval()\n",
    "\t# probability_outputs=[]\n",
    "\t# with torch.no_grad():\n",
    "\t# \tfor i, (inputs, labels) in tqdm(enumerate(test_dataloader, 0), leave=False):\n",
    "\t# \t\toutputs = model(inputs)\n",
    "\t# \t\tprobability_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
    "\t# probability_outputs = np.array(probability_outputs)\n",
    "   \n",
    "\t# binary_outputs = (probability_outputs >= 0.5)\n",
    "\t# binary_outputs[np.all(binary_outputs == False, axis=1), -1] = True\n",
    "\n",
    "\t# # Calculate Result\n",
    "\t# report = classification_report(data_dict[data_type]['test']['label'], binary_outputs, output_dict=True)\n",
    "\t# df_report = pd.DataFrame(report).transpose()\n",
    "\t# df_report.reset_index(inplace=True)\n",
    "\t# df_report.rename(columns={'support': 'tag_count', 'index': 'tag'}, inplace=True)\n",
    "\t# df_report.loc[:num_intents-1, 'tag'] = intents_dict[data_type]\n",
    "\t# df_report[['precision', 'recall', 'f1-score']] = df_report[['precision', 'recall', 'f1-score']].round(4)\n",
    "\n",
    "\t# print(df_report)\n",
    "\n",
    "\t# Find best threshold\n",
    "\tmodel.eval()\n",
    "\tlabels_list = []\n",
    "\tprobability_outputs=[]\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, data in tqdm(enumerate(val_dataloader, 0), leave=False):\n",
    "\t\t\tids = data['ids'].to(device, dtype = torch.int)\n",
    "\t\t\tmask = data['mask'].to(device, dtype = torch.int)\n",
    "\t\t\ttargets = data['targets'].to(device, dtype = torch.float)\n",
    "\t\t\toutputs = model(ids, mask)\t\t\n",
    "\t\t\tlabels_list.extend(targets.cpu().detach().numpy().tolist())\n",
    "\t\t\tprobability_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
    "\n",
    "\tlabels = np.array(labels_list)\n",
    "\tprobability_outputs = np.array(probability_outputs)\n",
    "\n",
    "\tbest_thresholds = np.zeros(num_intents)\n",
    "\tthresholds = [(i+1)/100 for i in range(100)]\n",
    "\tfor label_idx in range(num_intents):\n",
    "\t\tbest_label_f1 = 0.0\n",
    "\t\tfor threshold in thresholds:\n",
    "\t\t\tbinary_preds = (probability_outputs[:, label_idx] >= threshold).astype(int)\n",
    "\t\t\tf1 = f1_score(labels[:, label_idx], binary_preds)\n",
    "\t\t\tif f1 > best_label_f1:\n",
    "\t\t\t\tbest_label_f1 = f1\n",
    "\t\t\t\tbest_thresholds[label_idx] = threshold\n",
    "\n",
    "\tprint(best_thresholds)\n",
    "\n",
    "\t# Test with best thresholds\n",
    "\tmodel.eval()\n",
    "\tlabels_list = []\n",
    "\tprobability_outputs=[]\n",
    "\tfor i, data in tqdm(enumerate(test_dataloader, 0), leave=False):\n",
    "\t\tids = data['ids'].to(device, dtype = torch.int)\n",
    "\t\tmask = data['mask'].to(device, dtype = torch.int)\n",
    "\t\ttargets = data['targets'].to(device, dtype = torch.float)\n",
    "\t\toutputs = model(ids, mask)\t\t\n",
    "\t\tlabels_list.extend(targets.cpu().detach().numpy().tolist())\n",
    "\t\tprobability_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
    "\n",
    "\tlabels = np.array(labels_list)\n",
    "\tprobability_outputs = np.array(probability_outputs)\n",
    " \n",
    "\tbinary_outputs = (probability_outputs >= best_thresholds).astype(int)\n",
    "\tbinary_outputs[np.all(binary_outputs == False, axis=1), -1] = True\n",
    " \n",
    "\treport = classification_report(labels[:, :-1], binary_outputs[:, :-1], output_dict=True)\n",
    "\tdf_report = pd.DataFrame(report).transpose()\n",
    "\tdf_report.reset_index(inplace=True)\n",
    "\tdf_report.rename(columns={'support': 'tag_count', 'index': 'tag'}, inplace=True)\n",
    "\tdf_report.loc[:num_intents-2, 'tag'] = intents_dict[data_type][:-1]\n",
    "\tdf_report[['precision', 'recall', 'f1-score']] = df_report[['precision', 'recall', 'f1-score']].round(2)\n",
    "\tdf_report = df_report[df_report['tag'].apply(lambda x: x not in ['micro avg', 'weighted avg', 'samples avg'])]\n",
    "\n",
    "\tprint(df_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "634_condafalse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
